{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for testing convolutional neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nn_packages",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6bc3dd63a7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import all necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_packages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroot_numpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nn_packages"
     ]
    }
   ],
   "source": [
    "#Import all necessary packages\n",
    "from nn_packages import *\n",
    "from io_functions import *\n",
    "import numpy as np\n",
    "import root_numpy as rnp\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "#import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation,Input, Dense, Dropout, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import model_from_json, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Convolution2D, Convolution3D, Flatten, MaxPooling2D, MaxPooling3D, Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution3d_7 (Convolution3D)  (None, 10, 21, 21, 21)810         convolution3d_input_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_8 (Convolution3D)  (None, 3, 20, 20, 17) 603         convolution3d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_4 (MaxPooling3D)    (None, 3, 10, 10, 8)  0           convolution3d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 2400)          0           maxpooling3d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1000)          2401000     flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             1001        dense_12[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2403414\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ConvNet - 3D, 6/7\n",
    "#Just taking the ECAL data (24x24x25)\n",
    "cnn3d_1 = Sequential()\n",
    "cnn3d_1.add(Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='sigmoid'))\n",
    "cnn3d_1.add(Convolution3D(3, 2, 2, 5, activation='sigmoid'))\n",
    "cnn3d_1.add(MaxPooling3D())\n",
    "cnn3d_1.add(Flatten())\n",
    "\n",
    "#Dense layer\n",
    "cnn3d_1.add(Dense(1000, activation='sigmoid'))\n",
    "#cnn3d_1.add(Dropout(0.5))\n",
    "cnn3d_1.add(Dense(1, activation='sigmoid'))\n",
    "cnn3d_1.compile(loss='mse', optimizer='sgd')\n",
    "cnn3d_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_3 (Convolution2D)  (None, 10, 17, 17)    4010        convolution2d_input_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 10, 8, 8)      0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 640)           0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 10000)         6410000     flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 10000)         0           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             10001       dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6424011\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ConvNet - 2D, 7/7\n",
    "cnn2d_1 = Sequential()\n",
    "cnn2d_1.add(Convolution2D(10, 4, 4, input_shape = (25, 20, 20), activation='sigmoid'))\n",
    "cnn2d_1.add(MaxPooling2D())\n",
    "cnn2d_1.add(Flatten())\n",
    "\n",
    "#Dense layer\n",
    "cnn2d_1.add(Dense(10000, activation='sigmoid'))\n",
    "cnn2d_1.add(Dropout(0.25))\n",
    "cnn2d_1.add(Dense(1, activation='linear'))\n",
    "cnn2d_1.compile(loss='mse', optimizer='sgd')\n",
    "cnn2d_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Class definition of Generator which reads data from input file, splits it into training and validation sets and feeds\n",
    "it into the neural network to train and validate it.\n",
    "'''\n",
    "\n",
    "class RegGen:\n",
    "    '''\n",
    "    Data generator class for directory of h5 files\n",
    "    '''\n",
    "\n",
    "    def __init__(self, batch_size ,train_split=0.6,validation_split=0.2,test_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist=[]\n",
    "        for i in xrange(1,6):\n",
    "            for j in xrange(1,11):\n",
    "                self.filelist.append('/data/shared/LCD/New_Data_Shuffled/GammaEscan_%d_%d.h5'%(i,j)) \n",
    "        self.train_split = train_split\n",
    "        self.validation_split = validation_split\n",
    "        self.test_split = test_split\n",
    "        self.fileindex = 0\n",
    "        self.filesize = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def train(self,modeltype=3):\n",
    "        '''\n",
    "        Generate data for training only\n",
    "        '''\n",
    "        length = len(self.filelist)\n",
    "        #deleting the validation and test set filenames from the filelist\n",
    "        del self.filelist[int(np.floor((1 - self.train_split) * length)):]\n",
    "        return self.batches(modeltype)\n",
    "    def test(self, modeltype=3):\n",
    "        '''\n",
    "        Generate data for testing only\n",
    "        '''\n",
    "        length = len(self.filelist)\n",
    "        #deleting the train and validation set filenames from the filelist\n",
    "        del self.filelist[:int(np.floor((1 - self.test_split) * length)) + 1]\n",
    "        return self.batches(modeltype)\n",
    "    def validation(self, modeltype=3):\n",
    "        '''\n",
    "        Generate data for validation only\n",
    "        '''\n",
    "        length = len(self.filelist)\n",
    "        #modifying the filename list to only include files for validation set\n",
    "        self.filelist = self.filelist[int(np.floor(self.train_split*length+1)):\\\n",
    "                                      int(np.floor((self.train_split + self.validation_split) * length+1))]\n",
    "        return self.batches(modeltype)\n",
    "        \n",
    "    #The function which reads files to gather data until batch size is satisfied\n",
    "    def batch_helper(self, fileindex, position, batch_size):\n",
    "        '''\n",
    "        Reads files to gather data until batch size is satisfied, then yeilds\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex], 'r')\n",
    "        self.filesize = np.array(f['ECAL']).shape[0]\n",
    "\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data_ECAL = np.array(f['ECAL'][position : position + batch_size])\n",
    "            data_HCAL = np.array(f['HCAL'][position : position + batch_size])\n",
    "            target  = np.array(f['target'][position : position + batch_size][:,:,1])\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            return data_ECAL, data_HCAL, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "            data_ECAL = np.array(f['ECAL'][position:])\n",
    "            data_HCAL = np.array(f['HCAL'][position:])\n",
    "            target =  np.array(f['target'][position:][:,:,1])\n",
    "            f.close()\n",
    "            \n",
    "            if fileindex+1 < len(self.filelist):\n",
    "                if self.batch_size - data_ECAL.shape[0] > 0:\n",
    "                    while self.batch_size - data_ECAL.shape[0] > 0:\n",
    "\n",
    "                        if int(np.floor((self.batch_size - data_ECAL.shape[0]) / self.filesize)) == 0:\n",
    "                            number_of_files = 1\n",
    "                        else:\n",
    "                            number_of_files = int(np.ceil((self.batch_size-data_ECAL.shape[0]) / self.filesize))\n",
    "\n",
    "                        for i in xrange(0, number_of_files):\n",
    "                            # restart in file list in case we run out of files\n",
    "                            if fileindex + i + 1 > len(self.filelist):\n",
    "                                fileindex = -1 - i\n",
    "\n",
    "                            f = h5py.File(self.filelist[fileindex+i+1],'r')\n",
    "\n",
    "                            if (self.batch_size - data_ECAL.shape[0] < self.filesize):\n",
    "                                position = self.batch_size - data_ECAL.shape[0]\n",
    "                                data_temp_ECAL = np.array(f['ECAL'][:position])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'][:position])\n",
    "                                target_temp = np.array(f['target'][:position][:,:,1])\n",
    "\n",
    "                            else:\n",
    "                                data_temp_ECAL = np.array(f['ECAL'])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'])\n",
    "                                target_temp = np.array(f['target'][:,:,1])\n",
    "\n",
    "                            f.close()\n",
    "                            data_ECAL = np.concatenate((data_ECAL, data_temp_ECAL), axis=0)\n",
    "                            data_HCAL = np.concatenate((data_HCAL, data_temp_HCAL), axis=0)\n",
    "                            target = np.concatenate((target, target_temp), axis=0)\n",
    "                    \n",
    "                    if (fileindex + i + 1 < len(self.filelist)):\n",
    "                        fileindex += i + 1\n",
    "                    else:\n",
    "                        fileindex = 0\n",
    "                else:\n",
    "                    position = 0\n",
    "                    fileindex += 1\n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data_ECAL, data_HCAL, target, fileindex, position\n",
    "    \n",
    "    def batches(self, modeltype):\n",
    "        '''\n",
    "        Loops indefinitely and continues to return data of specified batch size\n",
    "        '''\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data_ECAL,data_HCAL, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data_ECAL.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if modeltype==3:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(1, 24, 24, 25))\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(1, 4, 4, 60))\n",
    "\n",
    "            elif modeltype==2:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(24, 24, 25))\n",
    "                data_ECAL = np.swapaxes(data_ECAL, 1, 3)\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(4, 4, 60))\n",
    "                data_HCAL = np.swapaxes(data_HCAL, 1, 3)\n",
    "\n",
    "            elif modeltype==1:\n",
    "                data_ECAL= np.reshape(data_ECAL,(self.batch_size,-1))\n",
    "                data_HCAL= np.reshape(data_HCAL,(self.batch_size,-1))\n",
    "\n",
    "            yield (data_ECAL,target/500.) #not returning HCAL because we are not using HCAL for the above models\n",
    "        self.fileindex = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declaring objects of Data Generator to feed input from files as input dataset and validation set\n",
    "ds = RegGen(1000) #input dataset\n",
    "vs = RegGen(1000) #validation set\n",
    "early = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-54:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 763, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 425, in data_generator_task\n",
      "    generator_output = next(generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-55d473ae58f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#training the neural network and saving the history (training losses) in hist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn3d_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodeltype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodeltype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[0;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                         val_outs = self.evaluate_generator(validation_data,\n\u001b[0;32m   1470\u001b[0m                                                            \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m                                                            max_q_size=max_q_size)\n\u001b[0m\u001b[0;32m   1472\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m                         \u001b[1;31m# no need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[0;32m   1540\u001b[0m                 raise Exception('output of generator should be a tuple '\n\u001b[0;32m   1541\u001b[0m                                 \u001b[1;34m'(x, y, sample_weight) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1542\u001b[1;33m                                 'or (x, y). Found: ' + str(generator_output))\n\u001b[0m\u001b[0;32m   1543\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1544\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None"
     ]
    }
   ],
   "source": [
    "#training the neural network and saving the history (training losses) in hist\n",
    "hist = cnn3d_1.fit_generator(ds.train(modeltype=3), samples_per_epoch=2000, nb_epoch=5, validation_data= vs.validation(modeltype=3), nb_val_samples=2000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For prediction using the trained network\n",
    "filelist=[]\n",
    "#Making a filelist containing name of one of the input file\n",
    "\n",
    "filelist.append('/data/shared/LCD/New_Data_Shuffled/GammaEscan_1_1.h5')\n",
    "            \n",
    "for path in filelist:\n",
    "        f = h5py.File(path,'r')\n",
    "        data_ECAL = np.array(f['ECAL'])\n",
    "        data_HCAL = np.array(f['HCAL'])\n",
    "        test_target = np.array(f['target'][:,:,1])\n",
    "        f.close()\n",
    "        \n",
    "        data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(1, 24, 24, 25))\n",
    "        data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(1, 4, 4, 60))\n",
    "        \n",
    "        pred = np.array(cnn3d_1.predict([data_ECAL])) #Just using ECAL\n",
    "        print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
